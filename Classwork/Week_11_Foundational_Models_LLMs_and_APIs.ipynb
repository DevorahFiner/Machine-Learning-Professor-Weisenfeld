{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8f82c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Welcome to Machine Learning - Week 11\n",
    "Instructor - Daniel Wiesenfeld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bcbcdf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Foundational Models, LLMs, and APIs\n",
    "\n",
    "## Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1b214",
   "metadata": {},
   "source": [
    "## Foundational Models\n",
    "A foundational model is a large-scale, pre-trained model that serves as a base for a wide range of downstream tasks. These models are trained on vast amounts of data and are designed to understand and generate human language, as well as other types of data. Foundational models are characterized by their general-purpose capabilities and are fine-tuned or adapted for specific tasks or domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c1757",
   "metadata": {},
   "source": [
    "### Key Characteristics of Foundational Models:\n",
    "1) **Pre-trained on Large Datasets:** They are trained on extensive and diverse datasets, which allows them to learn a wide range of patterns and knowledge.\n",
    "2) **General-purpose:** They can be used as a base for various tasks without needing to be trained from scratch for each new task. This enables zero-shot inference, few-shot infernce, and in-context learning.\n",
    "3) **Scalable:** They typically have a large number of parameters, making them capable of handling complex and nuanced data.\n",
    "4) **Adaptable:** They can be fine-tuned or adapted to perform specific tasks, often with much less data and computational resources than required for training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0f0b1",
   "metadata": {},
   "source": [
    "### Examples of Foundational Models:\n",
    "* **GPT-3:** A generative language model that can perform tasks such as text completion, translation, and question answering.\n",
    "* **BERT:** A model designed for understanding the context of words in a sentence, useful for tasks like sentiment analysis and named entity recognition.\n",
    "* **T5:** A model that converts all NLP tasks into a text-to-text format, making it highly versatile.\n",
    "* **CLIP:** A model that understands both images and text, enabling tasks like image classification and text-to-image generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11ca6e",
   "metadata": {},
   "source": [
    "### Use Cases:\n",
    "* **Language Understanding:** Sentiment analysis, named entity recognition, and text classification.\n",
    "* **Language Generation:** Text completion, summarization, and translation.\n",
    "* **Multi-modal Tasks:** Combining text and images for tasks like captioning and visual question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b47c6",
   "metadata": {},
   "source": [
    "### Training Process:\n",
    "* **Pre-training:** The model is trained on a large corpus of data using typically self-supervised learning techniques to capture general patterns and knowledge.\n",
    "* **Fine-tuning:** The pre-trained model may be further trained on task-specific data using supervised learning techniques to adapt it for specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d98b2f",
   "metadata": {},
   "source": [
    "### Key Concepts in Self-Supervised Learning for Foundational Models:\n",
    "#### Self-Supervised Learning:\n",
    "* **Definition:** A form of unsupervised learning where the model is trained to predict parts of the input data from other parts. This creates a form of supervision without the need for manually labeled data.\n",
    "* **Examples:** Masked Language Modeling (MLM), Autoregressive Modeling, and Next Sentence Prediction (NSP).\n",
    "Pre-training Tasks:\n",
    "\n",
    "#### Masked Language Modeling (MLM):\n",
    "* Used in models like BERT.\n",
    "* Randomly masks some tokens in the input and trains the model to predict the masked tokens.\n",
    "* **Example:** For the input \"The capital of [MASK] is Paris\", the model learns to predict \"France\".\n",
    "\n",
    "#### Autoregressive Modeling:\n",
    "* Used in models like GPT.\n",
    "* Trains the model to predict the next token in a sequence given the previous tokens.\n",
    "* **Example:** For the input \"The capital of France is\", the model learns to predict \"Paris\".\n",
    "\n",
    "#### Sequence-to-Sequence Modeling:\n",
    "* Used in models like T5.\n",
    "* Converts input sequences into output sequences, enabling tasks like translation and summarization.\n",
    "* **Example:** For the input \"Translate English to French: Hello\", the model learns to output \"Bonjour\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161727e",
   "metadata": {},
   "source": [
    "### Benefits of Self-Supervised Learning:\n",
    "\n",
    "* **Utilizes Large Unlabeled Datasets:** Can leverage vast amounts of text data available on the internet without the need for manual labeling.\n",
    "* **Learns Rich Representations:** Captures complex patterns and structures in the data, which can be fine-tuned for specific tasks.\n",
    "* **Scalability:** Can be scaled to very large models and datasets, enhancing the model's generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c33a6",
   "metadata": {},
   "source": [
    "## How Foundational Models are Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852f8b2",
   "metadata": {},
   "source": [
    "### Zero-Shot Inference\n",
    "**Definition:** Zero-shot inference refers to the ability of a model to perform tasks without any explicit training on the specific task or dataset.\n",
    "\n",
    "**Mechanism:**\n",
    "* Leverages the model's pre-trained knowledge.\n",
    "* Utilizes natural language prompts (and/or other modalities) to specify tasks.\n",
    "\n",
    "**Examples:**\n",
    "* GPT-3 answering trivia questions without being explicitly trained on trivia datasets.\n",
    "* CLIP identifying objects in images based on descriptive text.\n",
    "* Instructing a model to act as a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1104cc3",
   "metadata": {},
   "source": [
    "### Few-Shot Inference\n",
    "**Definition:** Few-shot inference allows a model to perform tasks by being provided with a small number of examples.\n",
    "\n",
    "**Mechanism:**\n",
    "* Utilizes a few examples to understand the task.\n",
    "* Examples serve as context for the model to generate responses or predictions.\n",
    "\n",
    "**Examples:**\n",
    "* GPT-3 writing a story when given a few example sentences.\n",
    "* BERT performing text classification with a few labeled examples.\n",
    "* An image model classifying a new type of object after being shown a few images of the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf984ba",
   "metadata": {},
   "source": [
    "### In-Context Learning\n",
    "**Definition:** In-context learning allows models to learn tasks from context provided in the input, rather than through explicit parameter updates. Both zero shot and few shot inference rely on in-context learning.\n",
    "\n",
    "**Mechanism:**\n",
    "* The model uses a few examples provided in the input prompt to infer the task and generate responses accordingly.\n",
    "* Can also instruct the model on how to behave and what to or not to output.\n",
    "* The field of \"prompt engineering\" is focused on optimizing prompts to yield the right outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3ea10",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "**Definition:** Fine-tuning is the process of taking a pre-trained model and continuing to train it for a specific task. This results in making small modifications to the model weights that make the model better at that particular task.\n",
    "\n",
    "**Purpose:**\n",
    "* Leverages the general knowledge already captured by the model during pre-training.\n",
    "* Requires significantly less data and computational resources compared to training a model from scratch.\n",
    "\n",
    "**Applications:**\n",
    "Sentiment analysis, text classification, named entity recognition, translation, image classification, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a51f31",
   "metadata": {},
   "source": [
    "**In-Context Learning:**\n",
    "* Does not update model parameters.\n",
    "* Relies on examples provided at inference time.\n",
    "* Flexible and quick for new tasks.\n",
    "\n",
    "**Fine-Tuning:**\n",
    "* Involves updating model parameters using task-specific training data.\n",
    "* Typically requires more data and computational resources.\n",
    "* Provides more tailored and potentially higher performance for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e517cf",
   "metadata": {},
   "source": [
    "###  Fine-Tuning Techniques\n",
    "**Full Fine-Tuning:** Adjusts all the parameters of the model.\n",
    "* Pros: High adaptability and potentially better performance.\n",
    "* Cons: Computationally expensive and risk of overfitting.\n",
    "\n",
    "**Partial Fine-Tuning:** Adjusts only a subset of the model parameters (e.g., last layer or a few layers).\n",
    "* Pros: Less computational resources and faster training.\n",
    "* Cons: Limited adaptability.\n",
    "\n",
    "**Parameter Efficient Fine-Tuning (PEFT):**\n",
    "* Techniques like Adapter layers, BitFit, and Quantization + Low-Rank Adaptation (Q-LoRA).\n",
    "* Adapter Layers\n",
    "    - Adding small trainable layers between pre-trained layers.\n",
    "* BitFit\n",
    "    - Fine-tuning only the bias terms in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca487d9",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG)\n",
    "**Definition:** Retrieval-Augmented Generation (RAG) enhances model performance by incorporating external information retrieval during inference.\n",
    "\n",
    "**Mechanism:** \n",
    "* Combines a retrieval model (e.g., Dense Passage Retrieval, DPR) with a generation model (e.g., BERT, GPT).\n",
    "* Retrieves relevant documents from a large corpus to provide additional context for the generation model.\n",
    "\n",
    "**Applications:**\n",
    "* Open-domain question answering.\n",
    "* Fact-checking and information retrieval.\n",
    "\n",
    "**Advantages:**\n",
    "* Improves accuracy by leveraging external knowledge.\n",
    "* Reduces the reliance on model's parameterized knowledge alone.\n",
    "* Provides a solution to limited context windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308e907",
   "metadata": {},
   "source": [
    "### Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "**Definition:** A method for aligning LLMs with human values and preferences by incorporating human feedback into the training process. \n",
    "\n",
    "**USE:** Used to fine-tune models for specific behaviors, ethical considerations, and instruction-following capabilities.\n",
    "\n",
    "**RLHF Process:**\n",
    "* Pre-training: Train a base model on a large dataset using self-supervised learning.\n",
    "* Fine-tuning: Adjust the model using human feedback on specific outputs.\n",
    "* Reward Modeling: Train a reward model to predict human preferences based on feedback.\n",
    "* Reinforcement Learning: Optimize the model to maximize the reward model's output.\n",
    "\n",
    "**Applications:**\n",
    "* Instruction following\n",
    "* Content moderation\n",
    "* Ethical AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec9fc0",
   "metadata": {},
   "source": [
    "### Techniques for Model Adaptation\n",
    "\n",
    "**Distillation**\n",
    "* Compressing a large model into a smaller one while retaining performance\n",
    "* Transfers knowledge from a large \"teacher\" model to a smaller \"student\" model.\n",
    "* Example: DistilBERT\n",
    "\n",
    "**Quantization**\n",
    "* Reducing the precision of model weights (e.g., from 32-bit floats to 8-bit floats).\n",
    "* Benefits: Smaller model size, faster inference.\n",
    "* Types:\n",
    "    - Static Quantization: Quantizes weights and activations based on a calibration dataset.\n",
    "    - Dynamic Quantization: Quantizes weights only, and activation values are quantized on-the-fly.\n",
    "\n",
    "**Low-Rank Adaptation (LoRA)**\n",
    "* Decomposing weight matrices into lower-rank matrices.\n",
    "* Reduces the number of parameters and computation required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36539387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27abc4cf880f4374a9ba82073128b9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\danie\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ac64aa40db45a18c0577fd95896ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ff560ef75a479d9e36c5e90333b14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9beb5d361bc477eacdaa9f774f882ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146e8ae654924d80b8b65cdd49d8ab12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0732e8f0c241a1b44c375467288c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042f996e741949618657c1dfef8acdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\danie\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Input length of input_ids is 11, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Once upon a time, in a galaxy far, far away\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Encode input text\n",
    "input_text = \"Once upon a time, in a galaxy far, far\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(inputs.input_ids, max_length=10, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da3d49",
   "metadata": {},
   "source": [
    "['audio-classification', 'automatic-speech-recognition', 'conversational', 'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text2text-generation', 'token-classification', 'translation', 'visual-question-answering', 'vqa', 'zero-shot-classification', 'zero-shot-image-classification', 'translation_XX_to_YY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45cf87bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I can't believe I'm doing this. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(task = \"text-generation\", model=\"openai-community/gpt2\")\n",
    "generator(\"I can't believe\", do_sample=False)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "81b74946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time in a galaxy filled with aliens, a woman was found by an android called the Doctor. This woman had a child while he was with the Doctor, and had been living on a farm with friends. She was called her sister so'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Once upon a time in a galaxy\")[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "904d9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!',\n",
       " 'labels': ['urgent', 'phone', 'tablet', 'computer', 'not urgent'],\n",
       " 'scores': [0.5578488111495972,\n",
       "  0.3911593556404114,\n",
       "  0.033464979380369186,\n",
       "  0.014779362827539444,\n",
       "  0.002747497521340847]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification', model=\"roberta-large-mnli\")\n",
    "classifier(\"I have a problem with my iphone that needs to be resolved asap!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14bc7a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       " 'labels': ['english', 'german'],\n",
       " 'scores': [0.7661928534507751, 0.23380716145038605]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"english\", \"german\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52ef9076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9995545744895935}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\", )\n",
    "\n",
    "# Analyze sentiment of a text\n",
    "sentiment(\"I love teaching machine learning!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fe8b1ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'La vie est comme une boîte de chocolats'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "en_fr_translator(\"life is like  a box of chocolates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3ea3fb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to microsoft/DialoGPT-medium and revision 8bada3b (https://huggingface.co/microsoft/DialoGPT-medium).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: d06e87ec-0419-4539-91f7-103381ccf76e \n",
       "user >> life is like  a box of chocolates \n",
       "bot >> I'm not sure if that's a good or a bad thing. "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "convo = pipeline(\"conversational\")\n",
    "convo([Conversation(\"life is like  a box of chocolates\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6385585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: db04c6c7-0c18-49f7-9205-ce910bc37834 \n",
       "user >> Could you tell me how to cook rice? \n",
       "bot >> I'm not sure, but I think you can use a rice cooker. "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo([Conversation(\"Could you tell me how to cook rice?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4592b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: b112e600-eac9-426f-b636-c16ca3fd7208 \n",
       "user >> Could you tell me how to cook rice? \n",
       "bot >> I'm not sure, but I think you can use a rice cooker. \n",
       "user >> Could I just use a plain old pot? \n",
       "bot >> I don't think so. I think you'd have to use a rice cooker. "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo([Conversation(\"Could I just use a plain old pot?\", \n",
    "                   past_user_inputs = [\"Could you tell me how to cook rice?\"], \n",
    "                   generated_responses = [\"I'm not sure, but I think you can use a rice cooker.\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "acb23858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: f805a4e5-d815-44cd-a076-9ee1febb556d \n",
       "user >> What should I have for breakfast? \n",
       "bot >> A sandwich "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo([Conversation(\"What should I have for breakfast?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e8caad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'M2M100Tokenizer'. \n",
      "The class this function is called from is 'SMALL100Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La vie est comme une boîte de chocolat.']\n",
      "['Life is like a box of chocolate.']\n",
      "['Life is like a box of chocolate.']\n",
      "['איך בין געגאנגען צו צוקולד.']\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/alirezamsh/small100\n",
    "\n",
    "from transformers import M2M100ForConditionalGeneration\n",
    "from tokenization_small100 import SMALL100Tokenizer\n",
    "\n",
    "hi_text = \"जीवन एक चॉकलेट बॉक्स की तरह है।\"\n",
    "chinese_text = \"生活就像一盒巧克力。\"\n",
    "hebrew_text = \"החיים כמו קופסה של שוקולדים\"\n",
    "english_text = \"life is like a box of chocolates\"\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"alirezamsh/small100\")\n",
    "tokenizer = SMALL100Tokenizer.from_pretrained(\"alirezamsh/small100\")\n",
    "\n",
    "# translate Hindi to French\n",
    "tokenizer.tgt_lang = \"fr\"\n",
    "encoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_hi)\n",
    "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "# => \"La vie est comme une boîte de chocolat.\"\n",
    "\n",
    "# translate Chinese to English\n",
    "tokenizer.tgt_lang = \"en\"\n",
    "encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_zh)\n",
    "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "# => \"Life is like a box of chocolate.\"\n",
    "\n",
    "# translate Hebrew to English\n",
    "tokenizer.tgt_lang = \"en\"\n",
    "encoded_he = tokenizer(hebrew_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_he)\n",
    "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "# => \"Life is like a box of chocolate.\"\n",
    "\n",
    "# translate Hebrew to Yiddish\n",
    "tokenizer.tgt_lang = \"yi\"\n",
    "encoded_en = tokenizer(english_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_en)\n",
    "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "# => \"Life is like a box of chocolate.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c1a54",
   "metadata": {},
   "source": [
    "*Source: https://huggingface.co/tasks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e66960",
   "metadata": {},
   "source": [
    "## Current Major Large Language Models:\n",
    "https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce081818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
